{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is it going to rain tomorrow?\n",
    "\n",
    "This project is for me to practice my machine learning chops while I'm in the early parts of my class. I'll be exploring the [Rain in Australia](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package) Kaggle dataset and attempt to build a model to predict whether or not it will rain the next day for a given observation. Let's see how it goes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from functools import partial\n",
    "from missingpy import MissForest\n",
    "from scipy import stats\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_predict, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder, OneHotEncoder, StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to actually download the data. Thankfully Kaggle provides a helpful [API](https://github.com/Kaggle/kaggle-api). This will be installed as part of the `requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if ! [ -d \"./data\" ]; then\n",
    "    mkdir ./data && cd ./data\n",
    "    kaggle datasets download jsphyg/weather-dataset-rattle-package --unzip\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/weatherAUS.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like `Evaporation`, `Sunshine`, `Cloud9am`, and `Cloud3pm` all have a lot of missing observations. I'm inclined to remove them since that's a decent chunk of data with some missing observations. But before I do that, let's just take a quick peak at how it relates to `RainTomorrow` just to see if there's any interesting relationship there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "sns.boxplot(x=\"RainTomorrow\", y=\"Evaporation\", data=data, ax=axs[0][0])\n",
    "sns.boxplot(x=\"RainTomorrow\", y=\"Sunshine\", data=data, ax=axs[0][1])\n",
    "sns.boxplot(x=\"RainTomorrow\", y=\"Cloud9am\", data=data, ax=axs[1][0])\n",
    "sns.boxplot(x=\"RainTomorrow\", y=\"Cloud3pm\", data=data, ax=axs[1][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like all of them seem to have a big impact on `RainTomorrow`, though it's hard to tell with `Evaporation` because of the wonky distribution. Let's do a test to compare the means of both groups to see. None of these variables look normal though, so we probably can't do a t-test, but let's take a closer look at the distribution of one case for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data[\"Evaporation\"][data[\"RainTomorrow\"] == \"Yes\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look quite normal to me, to further reinforce, we'll do a [D'Agostino-Pearson](https://en.wikipedia.org/wiki/D%27Agostino%27s_K-squared_test) test for normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.normaltest(data[\"Evaporation\"][data[\"RainTomorrow\"] == \"No\"].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely not normal, as expected. So we can't do a t-test, but a common non-parametric alternative is the [Mann-Whitney U test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test). So let's try that!\n",
    "\n",
    "I was only interested in `Evaporation`, but might as well do all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_yes = data[data[\"RainTomorrow\"] == \"Yes\"]\n",
    "rain_no = data[data[\"RainTomorrow\"] == \"No\"]\n",
    "\n",
    "def rain_tomorrow_mannwhitneyu_test(variable):\n",
    "    return stats.mannwhitneyu(rain_yes[variable].dropna(), rain_no[variable].dropna())\n",
    "\n",
    "print(f\"\"\"\n",
    "Evaporation: {rain_tomorrow_mannwhitneyu_test('Evaporation')}\n",
    "Sunshine: {rain_tomorrow_mannwhitneyu_test('Sunshine')}\n",
    "Cloud9am: {rain_tomorrow_mannwhitneyu_test('Cloud9am')}\n",
    "Cloud3pm: {rain_tomorrow_mannwhitneyu_test('Cloud3pm')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, those are some significant p-values. Just like I expected. Looks like `Evaporation` is also highly significant.\n",
    "\n",
    "Since they are so important to determining `RainTomorrow`, I want to keep them in. During preparation, I can try imputing as well as dropping `NaN` rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!mkdir -p ./images\n",
    "\n",
    "pair_plot = sns.pairplot(data, hue=\"RainTomorrow\")\n",
    "pair_plot.savefig(\"./images/pairplot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although I can pick out some patterns from these scatter plots, it seems like `Evaporation` is the most powerful delineator in terms of `RainTomorrow`. I'll want to keep that in mind moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, at this point I think I've done enough initial exploration and engineering. Let's go ahead and start preparing the data for machine learning and see how well it can perform.\n",
    "\n",
    "Let's begin with a little feature engineering. One thing to note in particular is how a lot of these variables have a morning and evening variant. Let's make a new column for each of these to represent the change in said variable.\n",
    "\n",
    "Another interesting feature could be how much `Evaporation` there was for how much `Sunshine` there was in a day. My intuition tells me that if there was a lot of `Evaporation` and not a lot of `Sunshine` (it was already a cloudy day), the chance of it raining would be higher.\n",
    "\n",
    "Also if there was a lot of `Rainfall` and little `Sunshine`, it should continue to rain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(columns, X):\n",
    "    df = pd.DataFrame(X, columns=columns)\n",
    "    \n",
    "    df[\"ChangeInCloud\"] = df[\"Cloud3pm\"] - df[\"Cloud9am\"]\n",
    "    df[\"ChangeInHumidity\"] = df[\"Humidity3pm\"] - df[\"Humidity9am\"]\n",
    "    df[\"RangeOfTemp\"] = df[\"MaxTemp\"] - df[\"MinTemp\"]\n",
    "    df[\"ChangeInPressure\"] = df[\"Pressure3pm\"] - df[\"Pressure9am\"]\n",
    "    df[\"ChangeInTemp\"] = df[\"Temp3pm\"] - df[\"Temp9am\"]\n",
    "    df[\"ChangeInWindSpeed\"] = df[\"WindSpeed3pm\"] - df[\"WindSpeed9am\"]\n",
    "    \n",
    "    df[\"EvaporationPerSunshine\"] = df[\"Evaporation\"] / df[\"Sunshine\"]\n",
    "    df[\"RainfallPerSunshine\"] = df[\"Rainfall\"] / df[\"Sunshine\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll set up a pipeline for transforming the categorical and numeric variables accordingly.\n",
    "\n",
    "It's worth noting this will drop `RISK_MM`. Discussion [here](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package/discussion/78316) regarding `RISK_MM` as it involves data leakage.\n",
    "\n",
    "For `Date`, I want to split it into month and day for two _\"new\"_ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(estimator=None, imputer=(SimpleImputer(strategy=\"most_frequent\"), SimpleImputer()), engineer=True):\n",
    "    categorical_columns = [\"Location\", \"WindGustDir\", \"WindDir9am\", \"WindDir3pm\", \"RainToday\"]\n",
    "    numeric_columns = [\"MinTemp\", \"MaxTemp\", \"Rainfall\", \"Evaporation\", \"Sunshine\", \"WindGustSpeed\", \"WindSpeed9am\", \"WindSpeed3pm\",\n",
    "                       \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\", \"Temp9am\", \"Temp3pm\"]\n",
    "\n",
    "    date_transformer = Pipeline([\n",
    "        (\"split\", FunctionTransformer(lambda s: s.str.split('-', expand=True).iloc[:, 1:])),\n",
    "        (\"imputer\", imputer[0])\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline([\n",
    "        (\"imputer\", imputer[0]),\n",
    "        (\"one_hot\", OneHotEncoder(sparse=False))\n",
    "    ])\n",
    "    \n",
    "    numeric_transformer = Pipeline(list(filter(None, [\n",
    "        (\"imputer\", imputer[1]),\n",
    "        (\"add_one\", FunctionTransformer(lambda X: X + 1)),\n",
    "        (\"engineer\", FunctionTransformer(partial(engineer_features, numeric_columns))) if engineer else None,\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])))\n",
    "\n",
    "    column_transformer = ColumnTransformer([\n",
    "        (\"date\", date_transformer, \"Date\"),\n",
    "        (\"categorical\", categorical_transformer, categorical_columns),\n",
    "        (\"numeric\", numeric_transformer, numeric_columns)\n",
    "    ])\n",
    "    \n",
    "    return Pipeline(list(filter(None, [\n",
    "        (\"preprocessor\", column_transformer),\n",
    "        (\"estimator\", estimator) if estimator is not None else None\n",
    "    ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, I've set up our data preprocessing pipeline, now it's time to find a good model!\n",
    "\n",
    "To begin, I'll need to split our data into a train and test set and preprocess them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, data[\"RainTomorrow\"], test_size=0.2, stratify=data[\"RainTomorrow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preprocessor = LabelEncoder()\n",
    "y_train_p = y_preprocessor.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, I'll start by trying several different models on just the default parameters. Then I'll pick one to do some more in-depth training with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(),\n",
    "    Perceptron(),\n",
    "    SGDClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "]\n",
    "\n",
    "results = pd.DataFrame(columns=[\"Model\", \"Cross Validation Mean\"])\n",
    "for model in tqdm(models):\n",
    "    avg_score = np.mean(cross_val_score(pipeline(model), X_train, y_train_p, n_jobs=threads, verbose=1))\n",
    "    results.loc[len(results)] = [model.__class__.__name__, avg_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by=\"Cross Validation Mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"Cross Validation Mean\", y=\"Model\", data=results.sort_values(by=\"Cross Validation Mean\", ascending=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like I'll go with random forest since it has high accuracy and it's a model I'm familiar with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things I want to examine is the confusion matrix for these predictions. In particular with predicting rainfall, we may be more interested in recall, especially since on average it's more likely to not rain than to do rain, so the model can just lean on predicting _No_ and be highly accurate; but failing to predict rain can have more consequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pipeline(RandomForestClassifier())\n",
    "y_pred = cross_val_predict(clf, X_train, y_train_p, cv=10, n_jobs=threads, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(zip(y_preprocessor.transform(y_preprocessor.classes_), y_preprocessor.classes_)))\n",
    "sns.heatmap(confusion_matrix(y_train_p, y_pred), annot=True, fmt=\"d\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train_p, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was worried this would happen. The recall on the _Yes_ class where it does rain is only $50\\%$, that's horrible. I'll try re-preprocessing the data without features and imputation to see if how those had an effect.\n",
    "\n",
    "First, without imputing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped = data.dropna()\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(dropped, dropped[\"RainTomorrow\"], test_size=0.2, stratify=dropped[\"RainTomorrow\"])\n",
    "y_preprocessor_d = LabelEncoder()\n",
    "y_train_d_p = y_preprocessor_d.fit_transform(y_train_d)\n",
    "print(\"Number of observations:\", X_train_d.shape[0])\n",
    "\n",
    "\n",
    "clf_no_impute = pipeline(RandomForestClassifier())\n",
    "y_pred_no_impute = cross_val_predict(clf_no_impute, X_train_d, y_train_d_p, cv=10, n_jobs=threads, verbose=1)\n",
    "print(classification_report(y_train_d_p, y_pred_no_impute))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now without engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_no_engineer = pipeline(RandomForestClassifier(), engineer=False)\n",
    "y_pred_no_engineer = cross_val_predict(clf_no_engineer, X_train, y_train_p, cv=10, n_jobs=threads, verbose=1)\n",
    "print(classification_report(y_train_p, y_pred_no_engineer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now without either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_neither = pipeline(RandomForestClassifier(), engineer=False)\n",
    "y_pred_neither = cross_val_predict(clf_neither, X_train_d, y_train_d_p, cv=10, n_jobs=threads, verbose=1)\n",
    "print(classification_report(y_train_d_p, y_pred_neither))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like none of that really mattered, though removing imputation seemed to improve general performance slightly, but at the cost of a significant reduction in observations. I can try using a random forest to impute the data, however it is very slow. I'll see how well it works before committing to it. Hopefully, being able to keep a lot of data as well as better imputation will improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_test_rf = pipeline(RandomForestClassifier(), imputer=(SimpleImputer(strategy=\"most_frequent\"), MissForest(n_jobs=threads)))\n",
    "y_pred_test_rf = cross_val_predict(clf_test_rf, X_train, y_train_p, cv=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train_p, y_pred_test_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that didn't show any improvement at all over just the `SimpleImputer`. In fact, it seems to perform worse! I guess I won't bother with that. It's clear that imputation isn't helping or hindering here, so instead I should focus on hyperparameter optimization to improve recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now I'll run my model through a randomized parameter search and optimize for recall instead of accuracy. Of course this is based on my own assumption. If this were a real task, there would be a lot of work to determine what the problem we are trying to solve is to determine the correct metric to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    \"estimator__n_estimators\" : [10, 50, 100, 300, 500, 1000],\n",
    "    \"estimator__max_depth\": [3, 5, 10, 20, 30, 40, 50],\n",
    "    \"estimator__min_samples_split\": [2, 5, 10, 15, 30],\n",
    "    \"estimator__min_samples_leaf\": [1, 3, 5, 10, 15, 20],\n",
    "    \"estimator__max_features\": [\"auto\", \"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "clf_rs = RandomizedSearchCV(pipeline(RandomForestClassifier()), param_distributions, n_iter=100, scoring=\"recall\", cv=10, n_jobs=threads, verbose=1)\n",
    "clf_rs = clf_rs.fit(X_train, y_train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session(\"rain.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What I've learned\n",
    "\n",
    "* Perform a train/test split before doing EDA.\n",
    "* Perform more EDA to gain more insight into relationships between variables.\n",
    "* Make options for pipeline parameters so they can be included as part of the parameter search.\n",
    "* Consider a different option for comparing multiple models.\n",
    "* Consider an ensemble method like a voting classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
